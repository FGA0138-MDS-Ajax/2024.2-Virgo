{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLzZ9aPKkQbw"
      },
      "source": [
        "Testing a google tutorial on AI creation with tensorflow.\n",
        "\n",
        "Theoretically, this foxtrot thing should be\n",
        "somewhat\n",
        "easy\n",
        "\n",
        "we'll see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "tXrKzpNJkUl3",
        "outputId": "1053c621-9be9-487f-cfd5-dc5be751ffeb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nTop curses:\\n  1. Bravo Alpha Mike Foxtrot (AKA BAMF, pronounced bamfeh)\\n  2. Foxtrot\\n  3. (Romeo/Mike) Foxtrot Uniform\\n  4. Charlie Foxtrot\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Written by BCL0c, whose code is very damn terrible and stuff.\n",
        "#Made following these google's tutorials:\n",
        "#   1. https://www.tensorflow.org/datasets/keras_example\n",
        "#   2. https://www.tensorflow.org/tutorials/keras/classification#explore_the_data\n",
        "\n",
        "#also, this one seems useful to learn to convert the\n",
        "#foxtroted Uniform Mess they send us through tfds to something\n",
        "#\"visualizable\":\n",
        "#   1. https://www.tensorflow.org/datasets/overview\n",
        "\n",
        "#Aditionally, as we'll be mostly using plant_village to train this test\n",
        "#model, I'll leave a link to the dataset documentation:\n",
        "#   1. https://www.tensorflow.org/datasets/catalog/plant_village\n",
        "# And a link to the builder, which got the labels to this dataset more\n",
        "# thorougly documented:\n",
        "#   2. https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/datasets/plant_village/plant_village_dataset_builder.py\n",
        "\n",
        "\n",
        "#this !terrible code! uses Locvst patented (not really) cursing and cussing\n",
        "#system for documentation. If you see a NATO code floating around\n",
        "#any kind of explanation, be assured that was a curse and it got\n",
        "#censored to a NATO Phonetic Alphabet code.\n",
        "\n",
        "\"\"\"\n",
        "Top curses:\n",
        "  1. Bravo Alpha Mike Foxtrot (AKA BAMF, pronounced bamfeh)\n",
        "  2. Foxtrot\n",
        "  3. (Romeo/Mike) Foxtrot Uniform\n",
        "  4. Charlie Foxtrot\n",
        "\"\"\"\n",
        "\n",
        "#JuliettCharlieIndiaIndiaHotelNovember"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_kUsXJbjy9N",
        "outputId": "84d2deb5-2c80-4049-dad2-a26af9a258aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.17.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#importing tensorflow, else we ain't using tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#we need this foxtrot here to import our dataset\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#importing graphing libs else we ain't graphing stuff and whatnots\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "print(tf.__version__) #verify if this piece of ass is alright."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACCClhTu0sCp"
      },
      "source": [
        "imports seem to work alright.\n",
        "Next step is to take a look at our\n",
        "data!\n",
        "\n",
        "Therefore, we need to:\n",
        "1. download this mf from tsds\n",
        "2. follow tutorial as planned, since we solved the problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBvO6iVJsKix",
        "outputId": "da789b95-c66d-4aa6-e558-59b93f28e036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "#first, we gather our data. !Fortunately! plantvillage's dataset\n",
        "#is already embeded in this lib\n",
        "\n",
        "#!line bellow does not work, plant_village's dataset is in tfds!\n",
        "#plants_dataset = keras.datasets.plant_village\n",
        "\n",
        "#as plant_village only got a single split,\n",
        "#we'll be splitting train into two other datasets\n",
        "#so we got something to train with...\n",
        "#also, Bravo Alpha line of code, so it got split\n",
        "(plant_dataset_train, plant_dataset_test), plant_info = tfds.load('plant_village',\n",
        "           split = ['train[:90%]','train[90%:]'],\n",
        "           shuffle_files= True,\n",
        "           as_supervised= True,\n",
        "           with_info= True\n",
        "          )\n",
        "\n",
        "assert isinstance(plant_dataset_train, tf.data.Dataset)\n",
        "assert isinstance(plant_dataset_test, tf.data.Dataset)\n",
        "print(plant_dataset_test)\n",
        "print(plant_dataset_train)\n",
        "\n",
        "#only 1 of the following seems to work at a time\n",
        "#len(plant_dataset_test) #-> about 5k images\n",
        "#len(plant_dataset_train) #-> about 48k images\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uth6M54zxQD"
      },
      "source": [
        "alright, simply getting the dataset\n",
        "seems to work fine.\n",
        "Though it takes a long Alpha time to actually download and extract this piece of Sierra. Kind of lousy, i really hope i don't need to do this every bloody time.\n",
        "\n",
        "Next step would be to explore it. Actually, will be, as of 11122024_0042\n",
        "\n",
        "This is the next day, or more acuratelly, the same day, i simply slept.\n",
        "That's that.\n",
        "\n",
        "We might have some kind of bug with our dataset download or it might be my lame connection. At least it seems to be downloading alright.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvK-zvsYtyxf"
      },
      "outputs": [],
      "source": [
        "#now, let's try to extract an example from our dataset with tfds.\n",
        "#runs blazing fast after we finally bloody download our datasets.\n",
        "\n",
        "\n",
        "#lines below run, but take a long ass time, so lets not.\n",
        "#example = tfds.visualization.show_examples(plant_dataset_test, plant_info)\n",
        "#plot.show(example) #theoretically working"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83_gJTEq3Byd"
      },
      "source": [
        "The dataset seems to be loaded just fine.\n",
        "\n",
        "We now should be able to test if\n",
        "it loads to training and stuff.\n",
        "\n",
        "AAAAnd, later, the initial loading script\n",
        "must be refined so we have some kind of\n",
        "testing dataset.\n",
        "Else, we're mostly foxtroted. Foxtrot Hotel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-PCPz-H7nWX"
      },
      "outputs": [],
      "source": [
        "#takes ages more time than above code. Not good...\n",
        "#It seems to be somewhat hard to explore the data gathered from tfds\n",
        "\n",
        "#type(plant_dataset_train)\n",
        "#dir(plant_dataset_train)\n",
        "#tfds.as_dataframe(plant_dataset_train.take(10), plant_info)\n",
        "\n",
        "#not bad, but it seems this mikefoxtrot got as many labels as i got\n",
        "#hairs where the sun ain't shinning. Not great."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Kq9AcxV5Vz"
      },
      "source": [
        "Below sits the normalization steps.\n",
        "\n",
        "According to the tutorial this must be done since\n",
        "tlds provides images on a different encoding from\n",
        "what's required by actual tf.\n",
        "Very bizarre.\n",
        "I'm not sure what they meant by doing this, which\n",
        "simply takes more processing time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU09pK7IfKsD"
      },
      "outputs": [],
      "source": [
        "#blazing fast\n",
        "#according to the end-to-end-keras tutorial on the tfds\n",
        "#we should normalize the images and stuff.\n",
        "#Of course, i don't understand the underlying nuances,\n",
        "#but i guess this is \"prolly\" needed.\n",
        "\n",
        "#thankfully, tf got a tool for that.\n",
        "\n",
        "#ah yeah, most of this was suggested by the AI, saved me some time\n",
        "#to copy from the tutorial...\n",
        "\n",
        "def normalize(image,label):\n",
        "  return tf.cast(image, tf.float32)/255., label\n",
        "\n",
        "plant_dataset_train = plant_dataset_train.map(\n",
        "    normalize,\n",
        "    num_parallel_calls = tf.data.AUTOTUNE\n",
        "  )\n",
        "plant_dataset_train = plant_dataset_train.cache()\n",
        "plant_dataset_train = plant_dataset_train.shuffle(\n",
        "    plant_info.splits['train'].num_examples\n",
        "    )\n",
        "\n",
        "#don't know why this MikeFoxtrot suggested a 32, yet he did, so let's trust the\n",
        "#rogue AI for a sec here. The tutorial suggested a 128.\n",
        "plant_dataset_train = plant_dataset_train.batch(32)\n",
        "plant_dataset_train = plant_dataset_train.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYQUhJUYynIT"
      },
      "outputs": [],
      "source": [
        "#same treatment done to the testing dataset.\n",
        "\n",
        "plant_dataset_test = plant_dataset_test.map(\n",
        "    normalize,\n",
        "    num_parallel_calls = tf.data.AUTOTUNE\n",
        "  )\n",
        "plant_dataset_test = plant_dataset_test.cache()\n",
        "plant_dataset_test = plant_dataset_test.batch(32)\n",
        "plant_dataset_test = plant_dataset_test.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geB2nPKNy8-q",
        "outputId": "8fd72daa-5389-43b4-c0e6-d396f39efbf9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "#now, we'll try to create a model and stuff.\n",
        "\n",
        "#ah yeah, i have no clue on what works for this kind of\n",
        "#input and the desired output, so we'll prolly be\n",
        "#throwing sierra on the wall and seeing what\n",
        "#sticks, eh?\n",
        "\n",
        "#this here explains what the hotel is a conv2d:\n",
        "# 1. https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148\n",
        "\n",
        "#ah yeah, this is the label number for plant_village\n",
        "#i had to count it. MANUALLY.\n",
        "num_labels = 38 # YO LMFAO #-> this comment was made by renurin.\n",
        "\n",
        "#mein negus (renuringer) forgor to complete this guy. -> Mb (renurin)\n",
        "#will do as soon as i get home :/\n",
        "#theoretically!!!!\n",
        "\n",
        "model1 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape = (256,256,3)),\n",
        "    tf.keras.layers.Dense(128),\n",
        "    tf.keras.layers.Dense(num_labels)\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "vaBofTPaU-gF",
        "outputId": "a1bb5127-5e92-40e5-c04c-e82319723bd0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196608</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,165,952</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,902</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196608\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │      \u001b[38;5;34m25,165,952\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)                  │           \u001b[38;5;34m4,902\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,170,854</span> (96.02 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,170,854\u001b[0m (96.02 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,170,854</span> (96.02 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,170,854\u001b[0m (96.02 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Renurin (too lazy)\n",
        "# Gotta optimize with tf.optimize somehow\n",
        "# This compiles the model, but its not trained\n",
        "model1.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "model1.summary() #very cool, we alocated bloody 3.02 gb in less than a sec!\n",
        "# Renurin asks:\n",
        "# yo what happened to the directories y r there 1000000000?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qW775xvWYPuX",
        "outputId": "bbda9e23-2c32-4686-b203-6f90406b4b0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "# Smth has to be done so trying to train\n",
        "# Done by Renurin (bored)\n",
        "logdir = 'logs'\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "hist = model1.fit(\n",
        "    plant_dataset_train,\n",
        "    epochs=10,\n",
        "    validation_data=plant_dataset_test,\n",
        "    callbacks=[tensorboard_callback])\n",
        "# not sure what happened here but okay the ai autocompleted so......\n",
        "hist.history\n",
        "\n",
        "# Getting the perfomance\n",
        "fig= plot.figure()\n",
        "plot.plot(hist.history['accuracy'],color='pink', label='accuracy')\n",
        "plot.plot(hist.history['val_accuracy'], color = 'yellow', label = 'val_accuracy')\n",
        "plot.xlabel('Epoch')\n",
        "plot.ylabel('Accuracy')\n",
        "plot.ylim([0.5, 1])\n",
        "plot.legend(loc='upper right')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7rjUxGQbyzI"
      },
      "source": [
        "### Hitting the evaluations or smth\n",
        "Theoretically, gotta check the imports and dependencies and run the\n",
        "**%pip install ......**\n",
        "\n",
        "But for now, this is it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO1vmqiGYWd6"
      },
      "outputs": [],
      "source": [
        "# Renurin (bored) again\n",
        "# Now its time to evaluate the performance of the supposed model\n",
        "\n",
        "#commented because simply NOT READY EVEN FOR TESTING.\n",
        "# kay\n",
        "'''\n",
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()\n",
        "for batch in plant_dataset_test.as_numpy_iterator():\n",
        "  X, y = batch\n",
        "  yhat = model1.predict(X)\n",
        "  pre.update_state(y, yhat)\n",
        "  re.update_state(y, yhat)\n",
        "  acc.update_state(y, yhat)\n",
        "print(f'Precision: {pre.result().numpy()},Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}')'''\n",
        "# Renurin (bad thoughts, very bad thoughts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQNSnc1j4ho5"
      },
      "source": [
        "## Now its time for the worst part: **Testing**\n",
        "We better plan whats gonna happen **before** actually coding anything.\n",
        "- Too lazy to check the achitecture doc\n",
        "- Too lazy to read\n",
        "\n",
        "What is _actually_ happening: Make some prototype and adapt it accordingly to whatever the AI needs to be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp1ZHNsY_Aa0"
      },
      "outputs": [],
      "source": [
        "# Renurin (somewhat (in)sane)\n",
        "# This is future code, not for now\n",
        "import cv2\n",
        "# Grab an image that the models never seen before\n",
        "new_img = cv2.imread('new_image')\n",
        "plot.imshow(cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB))\n",
        "plot.show()\n",
        "# Gotta resize to the labels we established which I belive are 256x256\n",
        "resize = tf.image.resize(new_img, (256,256))\n",
        "plot.imshow(resize.numpy().astype(int))\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPCu7BJkq1GE"
      },
      "outputs": [],
      "source": [
        "# Apparently to test it we need to encapsulate it into another array\n",
        "resize = tf.expand_dims(resize, 0).shape\n",
        "yhat = model1.predict(np.expand_dims(resize/255,0))\n",
        "print(yhat)\n",
        "# So now we can predict smth but the main thing is the 38 labels. Im sure this works for 2 labels, but can it do the same?\n",
        "# Also, gotta save the model but not sure how\n",
        "# COULD define number intervals for the labels, I belive this will mess up the accuracy though"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pU035v6snp-"
      },
      "source": [
        "## Trying to save the model\n",
        "Just to clarify this is happening only after the model works or the deadline knocks on our door. However I prefer to establish some steps to be done just to make sure we got smth"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
